
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
<style>
 @font-face {
    font-family: 'poppins';
    src: url('Poppins-Regular.ttf');
  }

* {
    margin: 0;
    padding: 0;
}
html {
    height: 100vh;
    font-family: poppins;
    font-size: 3vh;
    color: rgb(87, 86, 86);
    background-color: whitesmoke;

}
header{
    display: flex;
    justify-content: space-around;
    height: 10%;
    background: #605B56;
}
header > div{
    margin: auto;
}
body{
    height: 100%;
    display: flex;
    flex-direction: column;
    justify-content: center;
    align-content: center;

}
.title{
    color: white;
}
main {
    display: flex;
    height: 90%;
    margin: 1vh;
}
ul {
    list-style: none;
}

details {
    margin: 2px;
}
.sectionsummary{
    font-weight: bold;
    list-style-type: none;
}
details > .sectionsummary{
    border: 1px solid;
    border-radius: 3px;
    list-style-type: '+';
}
details[open] > .sectionsummary{
    /* color: rgb(86, 86, 247); */
    color: #4BB3FD;
    list-style-type: '-';
}
details > .subsectionsummary{
    list-style-type: '+';
}
details[open] > .subsectionsummary{
    /* color: rgb(86, 86, 247); */
    /* color: #FF7F51; */
    color: #F38D68;
    list-style-type: '-';
}

details[open] > .keypointsummary{
    /* color: rgb(86, 86, 247); */
    color: #00A676;
}

.subsectionsummary{
    font-size: 2.25vh;
    list-style: none;
}

.pointslist {
    font-weight: bold;
    color: rgba(26, 26, 26, 0.631);
}

.keypoint{
    font-size: 2.25vh;
}

li{
    margin-left: 1vh;
}
li.summary{
    margin-left: 2vh;
    font-style: italic;
}

</style>
<title>ISTQB TAE Summary</title>
</head>
<body>
<header>
    <div class="title">ISTQB Test Automation Engineer Syllabus sections and their key points</div>
</header>
<main>
    <ul id="main-list" class="main-list">
        <li id="objectives" class="section">
            <details>
                <summary class="sectionsummary">Introduction and Objectives for Test Automation</summary>
                
<ul>
<li>
    <details class="subsection">
        <summary class="subsectionsummary">Purpose of Test Automation</summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary">Test automation is used for:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Using purpose built software tools to control and set up test preconditions</li><li class="summary">Executing tests</li><li class="summary">Comparing actual outcomes to predicted outcomes</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Testware consists of:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Software</li><li class="summary">Documentation</li><li class="summary">Test cases</li><li class="summary">Test environment</li><li class="summary">Test data</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Interaction with a SUT via:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">API (low-level)</li><li class="summary">GUI (high-level)</li><li class="summary">Service or protocol</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Test automation objectives:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Improving test efficiency</li><li class="summary">Providing wider function coverage</li><li class="summary">Reducing total test cost</li><li class="summary">Performing tests that manual testers cannot</li><li class="summary">Shortening the test execution period</li><li class="summary">Increasing the test frequency/reducing the time required for test cycles</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Advantages of test automation:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">More tests can be run per build</li><li class="summary">The possibility to create tests 
                    that cannot be done manually (real-time, remote, parallel tests)</li><li class="summary">Tests can be more complex</li><li class="summary">Tests run faster</li><li class="summary">Tests are less subject to operator error</li><li class="summary">More effective and efficient use of testing resources</li><li class="summary">Quicker feedback regarding software quality</li><li class="summary">Improved system reliability (e.g., repeatability, consistency)</li><li class="summary">Improved consistency of tests</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Disadvantages of test automation:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Additional costs are involved</li><li class="summary">Initial investment to setup TAS</li><li class="summary">Requires additional technologies</li><li class="summary">Team needs to have development and automation skills</li><li class="summary">On-going TAS maintenance requirement</li><li class="summary">Can distract from testing objectives, 
                    e.g., focusing on automating tests cases at the expense of
                    executing tests</li><li class="summary">Tests can become more complex</li><li class="summary">Additional errors may be introduced by automation</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Limitations of test automation:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Not all manual tests can be automated</li><li class="summary">The automation can only check machine-interpretable results</li><li class="summary">The automation can only check actual results that can be verified by an automated test oracle</li><li class="summary">Not a replacement for exploratory testing</li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<li>
    <details class="subsection">
        <summary class="subsectionsummary">Success Factors in Test Automation</summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary">Test Automation Architecture (TAA)</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Closely aligned with the architecture of a software product.</li><li class="summary">Designed for maintainability, performance and learnability.</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">SUT testability</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Should be considered as early as possible during SUT design</li><li class="summary">Requires effort to increase, e.g, by adding new interfaces</li><li class="summary">The testable parts of the SUT should be targeted first by automation</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Test automation strategy</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Should be practical and consistent</li><li class="summary">May vary depending on the part of the SUT</li><li class="summary">May include both UI and API tests to check test results consistency</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Test automation framework must:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">be easy to use</li><li class="summary">be well documented</li><li class="summary">be maintainable</li><li class="summary">support a consistent approach to automating tests</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">An easy to use and maintainable TAF qualities:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Good reporting and logging facilities: reports and logs should be full 
                    and usable by any member of the team(devs, testers, managers)</li><li class="summary">Easy troubleshooting: SUT, TAS or environment bugs recognition</li><li class="summary">Test environment control for test consistency</li><li class="summary">Detailed documentation</li><li class="summary">Good test cases tracing</li><li class="summary">Changes in the SUT require minimum changes in the TAF</li><li class="summary">High automated testware reuse</li><li class="summary">Tests are up-to-date and retired as needed</li><li class="summary">Deployment is well planned</li><li class="summary">Can recover if SUT encounters a fatal error</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Things to avoid to succeed:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Code sensitive to interface changes</li><li class="summary">Automation sensitive to data changes</li><li class="summary">Environment sensitive to context</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Advice:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Not all success factors can be met early on</li><li class="summary">Once the TAA is in place, continuos improvement is needed to meet
                    as many factors as possible</li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<ul>

            </details>
        </li>
        <li id="preparation" class="section">
            <details>
                <summary class="sectionsummary">Preparing for Test Automation</summary>
                
<ul>
<li>
    <details class="subsection">
        <summary class="subsectionsummary">SUT Factors Influencing Test Automation</summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary">SUT interfaces</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Control interfaces</li><li class="summary">Monitor interfaces</li><li class="summary">High or low level</li><li class="summary">A particular level automation should only be carried out
                    when this level interfaces are proved to be workng correctly</li><li class="summary">When an interface is not ready, it can be mocked or stubbed</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Third party software</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Sometimes third party software used by SUT may require testing</li><li class="summary">May require different test approach than the SUT itself</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Levels of intrusion</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">The greater the number of changes that are required to be made to the SUT 
                specifically for automated testing, the higher the level of intrusion.</li><li class="summary">The lower the intrusion level the better</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Different SUT architectures</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Different SUT architectures may require different test approaches (C++ vs Python)</li><li class="summary">The differences may be evened out with hibrid strategy</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Size and complexity of the SUT</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">The larger the SUT the more comprehensive TAS is needed</li><li class="summary">No need of a large TAS for a small SUT</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">When the SUT does not yet exist test automation planning can start</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Select automation candidates based on the requirements</li><li class="summary">Think of a strategy and approach based on the candidates</li><li class="summary">Propose test interfaces to be added to the SUT</li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<li>
    <details class="subsection">
        <summary class="subsectionsummary">Tool Evaluation and Selection</summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary">TAE contribution to choosing the right tool</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Assessing organizational maturity and identification of opportunities for test tool support</li><li class="summary">Assessing appropriate objectives for test tool support</li><li class="summary">Identifying and collecting information on potentially suitable tools</li><li class="summary">Analyzing tool information against objectives and project constraints</li><li class="summary">Estimating the cost-benefit ratio based on a solid business case</li><li class="summary">Making a recommendation on the appropriate tool</li><li class="summary">Identifying compatibility of the tool with SUT components</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">General tool advice:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Use well-known tools</li><li class="summary">Get familiar with the tool before using it in the TAS</li><li class="summary">Make sure tool has all the capabilities necessary</li><li class="summary">Make sure tool does not have too much functions that you will not need</li><li class="summary">Read tool's release notes</li><li class="summary">Visit forums where the tool is discussed</li><li class="summary">Upgrade the tool as needed</li><li class="summary">Minimize the dependency on the tool</li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<li>
    <details class="subsection">
        <summary class="subsectionsummary">Design for Testability and Automation.</summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary">Design for testability parts:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Observability (insight into the system)</li><li class="summary">Control(ability)</li><li class="summary">Clearly defined architecture</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Software interfaces that support testing</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Spreadsheets scripting</li><li class="summary">Stubs and mocks</li><li class="summary">Software interfaces (instead of real devices): help to simulate hardware failures</li><li class="summary">Interfaces for SUT state querying</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Design for automation should consider that:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Compatibility with existing test tools should be established early on.</li><li class="summary">Test tool compatibility with the SUT details is critical</li><li class="summary">Solutions may require development of program code and calls to APIs</li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<ul>

            </details>
        </li>
        <li id="gtaa" class="section">
            <details>
                <summary class="sectionsummary">The Generic Test Automation Architecture</summary>
                
<ul>
<li>
    <details class="subsection">
        <summary class="subsectionsummary">Introduction to gTAA</summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary">gTAA defines :</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Concept space</li><li class="summary">Layers</li><li class="summary">Components</li><li class="summary">Interfaces</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">gTAA is vendor-neutral and allows for a structured and modular approach to building a test automation solution by:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Defining the concept space, layers, services, and interfaces of a TAS 
                    to enable the realization of TASs 
                    by in-house as well as by externally developed components</li><li class="summary">Supporting simplified components for the effective and efficient development of test automation</li><li class="summary">Re-using test automation components wherever necessary</li><li class="summary">Easing the maintenance and evolution of TASs</li><li class="summary">Defining the essential features for a user of a TAS</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">TAS consists of:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Test environment (and its artifacts)</li><li class="summary">Test suites (a set of test cases including test data).</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">TAF provides:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Support for the realization of the test environment</li><li class="summary">Useful tools</li><li class="summary">Test harnesses</li><li class="summary">Supporting libraries</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">TAA principles to follow:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Single responsibility (component has only one reason to change)</li><li class="summary">Extension (component is closed for modification, open for extension)</li><li class="summary">Replacement (two components with the same functionality must be easily changeable)</li><li class="summary">Component segregation (one simple specific component is better than a large general one)</li><li class="summary">Dependency inversion (higher level components should not depend on the concrete implementation;
                    the TAF component should not depend on the test details)</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">gTAA horizontal layers(can be absent from the specific TAA)</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Test generation</li><li class="summary">Test definition</li><li class="summary">Test execution</li><li class="summary">Test adaptation</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">gTAA also has interfaces for:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Configuration management</li><li class="summary">Project management</li><li class="summary">Test management</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Test Generation Layer</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Supports tools for:<br> 
                    * manual test case design<br>
                    * developing, capturing, or deriving test data<br>
                    * automatically generating test cases from models that define the SUT and/or its environment<br></li><li class="summary">Components in this layer are used to:<br>
                * edit and navigate test suite structures <br>
                * relate test cases to test objectives or SUT requirements <br>
                * document the test design <br></li><li class="summary">Capabilities for automated test generation include:<br>
                * ability to model the SUT, its environment, and/or the test system<br>
                * ability to define test directives and to configure/parameterize test generation algorithms<br>
                * ability to trace the generated tests back to the model (elements)</li><li class="summary">Condsiderations for implementation:<br>
                * selection of manual or automated test generation<br>
                * selection of ,for example, requirements-based, data-based, scenario-based or behavior-based test generation<br>
                * selection of test generation strategies <br>
                * choice of the test selection strategy (practical coverage criteria, weights, risk assessments, etc)<br>
                </li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Test Definition Layer</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Supports tools for:<br>
                * specifying test cases (at a high and/or low level)<br>
                * defining test data for low-level test cases<br>
                * specifying test procedures for a test case or a set of test cases<br>
                * defining test scripts for the execution of the test cases<br>
                * providing access to test libraries as needed (for example in keyword-driven approaches<br></li><li class="summary">Components in this layer are used to:<br>
                * partition/constrain, parameterize or instantiate test data<br>
                * specify test sequences or fully-fledged test behaviors (including control statements and
                expressions), to parameterize and/or to group them<br>
                * document the test data, test cases and/or test procedures
                </li><li class="summary">Condsiderations for implementation:<br>
                * selection of data-driven, keyword-driven, pattern-based or model-driven test definition<br>
                * selection of notation for test definition <br>
                * selection of style guides and guidelines for the definition of high quality tests<br>
                * selection of test case repositories (spreadsheets, databases, files, etc.)<br>
                </li><li class="summary">Examples of notation for test definition:<br>
                * tables, state-based notation, stochastic notation,<br>
                * dataflow notation, business process notation, scenario-based notation<br>
                * spreadsheets, domain-specific test languages<br>
                * the Testing and Test Control Notation<br>
                * the UML Testing Profile (UTP)<br></li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Test Execution Layer</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Supports tools for:<br>
                 * executing test cases automatically<br>
                 * logging the test case executions<br>
                 * reporting the test results<br></li><li class="summary">Components in this layer provide the following capabilities:<br>
                * set up and tear down the SUT for test execution<br>
                * set up and tear down test suites (i.e., set of test cases including test data)<br>
                * configure and parameterize the test setup<br>
                * interpret both test data and test cases and transform them into executable scripts<br>
                * instrument the test system and/or the SUT for (filtered) logging of test execution and/or for fault
                injection<br>
                * analyze the SUT responses during test execution to steer subsequent test runs<br>
                * validate the SUT responses (comparison of expected and actual results) for automated test case
                execution results<br>
                * control the automated test execution in time<br></li><li class="summary">Condsiderations for implementation:<br>
                * selection of the test execution tool<br>
                * selection of interpretation or compilation approach (depends on the tool)<br>
                * selection of the implementation paradigm (imperative, functional, object-oriented, scripting or a tool-specific technology)<br>
                * selection of helper libraries to ease test execution<br>
                </li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Test Adaptation Layer</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Supports tools for:<br>
                * controlling the test harness<br>
                * interacting with the SUT<br>
                * monitoring the SUT<br>
                * simulating or emulating the SUT environment<br></li><li class="summary">Provides the following functionality:<br>
                * mediating between the technology-neutral test definitions and the specific technology requirements
                    of the SUT and the test devices<br>
                * applying different technology-specific adaptors to interact with the SUT<br>
                * distributing the test execution across multiple test devices/test interfaces or executing tests locally<br></li><li class="summary">Condsiderations for implementation:<br>
                * selection of test interfaces to the SUT
                * selection of tools to stimulate and observe the test interfaces
                * selection of tools to monitor the SUT during test execution
                * selection of tools to trace test execution (e.g., including the timing of the test execution)
                </li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Configuration management of a TAS</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">May include:<br>
                    * test models<br>
                    * test definitions/specifications including test data, test cases and libraries<br>
                    * test scripts<br>
                    * test execution engines and supplementary tools and components<br>
                    * test adaptors for the SUT<br>
                    * simulators and emulators for the SUT environment<br>
                    * test results and test reports<br></li><li class="summary">Should allow easy switching between versions</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Project management of a TAS and test management</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">TAS should provide easy access to the metrics and other reports to show to the managers</li><li class="summary">TAS should accept the SDLC of the SUT</li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<li>
    <details class="subsection">
        <summary class="subsectionsummary">TAA Design</summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary">TA approach considerations</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Which activity or phase of the test process should be automated (test generation stage is unique to TA)</li><li class="summary">Which test level should be supported</li><li class="summary">Which type of test should be supported</li><li class="summary">Which test role should be supported</li><li class="summary">Which software product(p.line, p.family) should be supported</li><li class="summary">Which SUT technologies should be supported</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Things to take into account to design a TAA</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Capture requirements needed to define an appropriate TAA</li><li class="summary">Compare and contrast different design/architecture approaches</li><li class="summary">Identify areas where abstraction can deliver benefits</li><li class="summary">Understand SUT technologies and how these interconnect with the TAS</li><li class="summary">Understand the SUT environment</li><li class="summary">Time and complexity for a given testware architecture implementation</li><li class="summary">Ease of use for a given testware architecture implementation</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Ways of automating test cases</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">to transform test cases directly into automated test scripts</li><li class="summary">to design test procedures and transform them into automated test scripts</li><li class="summary">to use a tool to translate test procedures into automated test scripts</li><li class="summary">to use a tool that generates automated test procedures and/or translates the test scripts directly from models</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Approaches for Automating Test Cases</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Capture/playback:<br>
                pros:<br>
                * easy to set up<br>
                cons:<br>
                * hard to maintain<br>
                * extremely flacky<br>
                * can't start before the SUT is ready<br>
                </li><li class="summary">Linear scripting:<br>
                pros:<br>
                * little preparation needed<br>
                * programming skills are not required most of the time<br>
                cons:<br>
                * repetition <br>
                * longer and non-modular scripts <br>
                * the effort depends on the number of steps <br>
                * requires effort to get familiar with the tool or language<br>
                </li><li class="summary">Structured scripting:<br>
                pros:<br>
                * usage of script libraries<br>
                * less effort for new tests <br>
                cons:<br>
                * larger initial effort <br>
                * programming skills required <br>
                * requires effort to manage scripts library<br>
                </li><li class="summary">Data-driven testing:<br>
                pros:<br>
                * less effort to create new tests due to control scripts <br>
                * more variations => larger coverage<br>
                * allows test analysts to specify tests via data files<br>
                cons:<br>
                * negative cases may be missed<br>
                * data files need to be managed<br>
                </li><li class="summary">Keyword-driven testing:<br>
                differences from DD-scripting:<br>
                * data files are called "test definitions"<br>
                * only one control script<br>
                * includes both data and "instructions"(keywords)
                pros:<br>
                * less effort for creating new tests <br>
                * test analysts may create tests<br>
                * tests can be defined in the terms of the high level actions<br>
                * keywords provide an abstraction <br>
                cons:<br>
                * requires a lot of effort if the tool does not support it<br>
                * requires care when deciding which keywords to implement<br>
                </li><li class="summary">Process-driven scripting:<br>
                concept: higher level abstraction 
                (small keywords vs longer procedures) <br>
                pros:<br>
                * workflow perspective <br>
                cons:<br>
                * SUT processes may be hard to grasp and describe<br>
                * requires care to implement correct processes <br>
                </li><li class="summary">Model-based testing:<br>
                pros:<br>
                * allows generating tests for different systems and technologies<br>
                * models are future-safe<br>
                * models require small changes when the SUT changes<br>
                * test design is incorporated in the test generation<br>
                cons:
                * requires a lot of expertise<br>
                * requires a lot of effort to model SUT<br>
                * requires adjustment is the test process<br>
                * not yet a mainstream<br>
                * models need to be tested and verified<br>
                </li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Technical considerations of the SUT</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Interfaces of the SUT</li><li class="summary">SUT data</li><li class="summary">SUT configurations</li><li class="summary">SUT standards and legal settings</li><li class="summary">Tools and tool environments used to develop the SUT</li><li class="summary">Test interfaces in the software product</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Considerations for Development/QA Processes</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Test execution control requirements</li><li class="summary">Reporting requirements</li><li class="summary">Role and access rights</li><li class="summary">Established tool landscape</li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<li>
    <details class="subsection">
        <summary class="subsectionsummary">TAS Development</summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary"></summary>
        <ol>
        <div class="pointslist">
        <li class="summary"></li><li class="summary"></li><li class="summary"></li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<ul>

            </details>
        </li>
        <li id="demployment" class="section">
            <details>
                <summary class="sectionsummary">Deployment Risks and Contingencies</summary>
            </details>
        </li>
        <li id="reporting" class="section">
            <details>
                <summary class="sectionsummary">Test Automation Reporting and Metrics</summary>
            </details>
        </li>
        <li id="mantoauto" class="section">
            <details>
                <summary class="sectionsummary">Transitioning Manual Testing to an Automated Environment</summary>
            </details>
        </li>
        <li id="verification" class="section">
            <details>
                <summary class="sectionsummary">Verifying the TAS</summary>
            </details>
        </li>
        <li id="improvement" class="section">
            <details>
                <summary class="sectionsummary">Continuous Improvement</summary>
            </details>
        </li>
    </ul>
    
</main>
</body>

</html>