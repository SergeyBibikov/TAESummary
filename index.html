
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
<style>
 @font-face {
    font-family: 'poppins';
    src: url('Poppins-Regular.ttf');
  }

* {
    margin: 0;
    padding: 0;
}
html {
    height: 100vh;
    font-family: poppins;
    font-size: 3vh;
    color: rgb(87, 86, 86);
    background-color: whitesmoke;

}
header{
    display: flex;
    justify-content: space-around;
    height: 10%;
    background: #605B56;
}
header > div{
    margin: auto;
}
body{
    height: 100%;
    display: flex;
    flex-direction: column;
    justify-content: center;
    align-content: center;

}
.title{
    color: white;
}
main {
    display: flex;
    height: 90%;
    margin: 1vh;
}
ul {
    list-style: none;
}

details {
    margin: 2px;
}
.sectionsummary{
    font-weight: bold;
    list-style-type: none;
}
details > .sectionsummary{
    border: 1px solid;
    border-radius: 3px;
    list-style-type: '+';
}
details[open] > .sectionsummary{
    color: #4BB3FD;
    list-style-type: '-';
}
details > .subsectionsummary{
    list-style-type: '+';
}
details[open] > .subsectionsummary{
    color: #F38D68;
    list-style-type: '-';
}

details[open] > .keypointsummary{
    color: #00A676;
}

.subsectionsummary{
    font-size: 2.25vh;
    list-style: none;
}

.pointslist {
    font-weight: bold;
    color: rgba(26, 26, 26, 0.631);
}

.keypoint{
    font-size: 2.25vh;
}

li{
    margin-left: 1vh;
}
li.summary{
    margin-left: 2vh;
    font-style: italic;
}

</style>
<title>ISTQB TAE Summary</title>
</head>
<body>
<header>
    <div class="title">ISTQB Test Automation Engineer Syllabus sections and their key points</div>
</header>
<main>
    <ul id="main-list" class="main-list">
        <li id="objectives" class="section">
            <details>
                <summary class="sectionsummary">Introduction and Objectives for Test Automation</summary>
                
<ul>
<li>
    <details class="subsection">
        <summary class="subsectionsummary">Purpose of Test Automation</summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary">Test automation is used for:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Using purpose built software tools to control and set up test preconditions</li><li class="summary">Executing tests</li><li class="summary">Comparing actual outcomes to predicted outcomes</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Testware consists of:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Software</li><li class="summary">Documentation</li><li class="summary">Test cases</li><li class="summary">Test environment</li><li class="summary">Test data</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Interaction with a SUT via:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">API (low-level)</li><li class="summary">GUI (high-level)</li><li class="summary">Service or protocol</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Test automation objectives:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Improving test efficiency</li><li class="summary">Providing wider function coverage</li><li class="summary">Reducing total test cost</li><li class="summary">Performing tests that manual testers cannot</li><li class="summary">Shortening the test execution period</li><li class="summary">Increasing the test frequency/reducing the time required for test cycles</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Advantages of test automation:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">More tests can be run per build</li><li class="summary">The possibility to create tests 
                    that cannot be done manually (real-time, remote, parallel tests)</li><li class="summary">Tests can be more complex</li><li class="summary">Tests run faster</li><li class="summary">Tests are less subject to operator error</li><li class="summary">More effective and efficient use of testing resources</li><li class="summary">Quicker feedback regarding software quality</li><li class="summary">Improved system reliability (e.g., repeatability, consistency)</li><li class="summary">Improved consistency of tests</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Disadvantages of test automation:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Additional costs are involved</li><li class="summary">Initial investment to setup TAS</li><li class="summary">Requires additional technologies</li><li class="summary">Team needs to have development and automation skills</li><li class="summary">On-going TAS maintenance requirement</li><li class="summary">Can distract from testing objectives, 
                    e.g., focusing on automating tests cases at the expense of
                    executing tests</li><li class="summary">Tests can become more complex</li><li class="summary">Additional errors may be introduced by automation</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Limitations of test automation:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Not all manual tests can be automated</li><li class="summary">The automation can only check machine-interpretable results</li><li class="summary">The automation can only check actual results that can be verified by an automated test oracle</li><li class="summary">Not a replacement for exploratory testing</li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<li>
    <details class="subsection">
        <summary class="subsectionsummary">Success Factors in Test Automation</summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary">Test Automation Architecture (TAA)</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Closely aligned with the architecture of a software product.</li><li class="summary">Designed for maintainability, performance and learnability.</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">SUT testability</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Should be considered as early as possible during SUT design</li><li class="summary">Requires effort to increase, e.g, by adding new interfaces</li><li class="summary">The testable parts of the SUT should be targeted first by automation</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Test automation strategy</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Should be practical and consistent</li><li class="summary">May vary depending on the part of the SUT</li><li class="summary">May include both UI and API tests to check test results consistency</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Test automation framework must:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">be easy to use</li><li class="summary">be well documented</li><li class="summary">be maintainable</li><li class="summary">support a consistent approach to automating tests</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">An easy to use and maintainable TAF qualities:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Good reporting and logging facilities: reports and logs should be full 
                    and usable by any member of the team(devs, testers, managers)</li><li class="summary">Easy troubleshooting: SUT, TAS or environment bugs recognition</li><li class="summary">Test environment control for test consistency</li><li class="summary">Detailed documentation</li><li class="summary">Good test cases tracing</li><li class="summary">Changes in the SUT require minimum changes in the TAF</li><li class="summary">High automated testware reuse</li><li class="summary">Tests are up-to-date and retired as needed</li><li class="summary">Deployment is well planned</li><li class="summary">Can recover if SUT encounters a fatal error</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Things to avoid to succeed:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Code sensitive to interface changes</li><li class="summary">Automation sensitive to data changes</li><li class="summary">Environment sensitive to context</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Advice:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Not all success factors can be met early on</li><li class="summary">Once the TAA is in place, continuos improvement is needed to meet
                    as many factors as possible</li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<ul>

            </details>
        </li>
        <li id="preparation" class="section">
            <details>
                <summary class="sectionsummary">Preparing for Test Automation</summary>
                
<ul>
<li>
    <details class="subsection">
        <summary class="subsectionsummary">SUT Factors Influencing Test Automation</summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary">SUT interfaces</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Control interfaces</li><li class="summary">Monitor interfaces</li><li class="summary">High or low level</li><li class="summary">A particular level automation should only be carried out
                    when this level interfaces are proved to be workng correctly</li><li class="summary">When an interface is not ready, it can be mocked or stubbed</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Third party software</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Sometimes third party software used by SUT may require testing</li><li class="summary">May require different test approach than the SUT itself</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Levels of intrusion</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">The greater the number of changes that are required to be made to the SUT 
                specifically for automated testing, the higher the level of intrusion.</li><li class="summary">The lower the intrusion level the better</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Different SUT architectures</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Different SUT architectures may require different test approaches (C++ vs Python)</li><li class="summary">The differences may be evened out with hibrid strategy</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Size and complexity of the SUT</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">The larger the SUT the more comprehensive TAS is needed</li><li class="summary">No need of a large TAS for a small SUT</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">When the SUT does not yet exist test automation planning can start</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Select automation candidates based on the requirements</li><li class="summary">Think of a strategy and approach based on the candidates</li><li class="summary">Propose test interfaces to be added to the SUT</li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<li>
    <details class="subsection">
        <summary class="subsectionsummary">Tool Evaluation and Selection</summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary">TAE contribution to choosing the right tool</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Assessing organizational maturity and identification of opportunities for test tool support</li><li class="summary">Assessing appropriate objectives for test tool support</li><li class="summary">Identifying and collecting information on potentially suitable tools</li><li class="summary">Analyzing tool information against objectives and project constraints</li><li class="summary">Estimating the cost-benefit ratio based on a solid business case</li><li class="summary">Making a recommendation on the appropriate tool</li><li class="summary">Identifying compatibility of the tool with SUT components</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">General tool advice:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Use well-known tools</li><li class="summary">Get familiar with the tool before using it in the TAS</li><li class="summary">Make sure tool has all the capabilities necessary</li><li class="summary">Make sure tool does not have too much functions that you will not need</li><li class="summary">Read tool's release notes</li><li class="summary">Visit forums where the tool is discussed</li><li class="summary">Upgrade the tool as needed</li><li class="summary">Minimize the dependency on the tool</li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<li>
    <details class="subsection">
        <summary class="subsectionsummary">Design for Testability and Automation.</summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary">Design for testability parts:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Observability (insight into the system)</li><li class="summary">Control(ability)</li><li class="summary">Clearly defined architecture</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Software interfaces that support testing</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Spreadsheets scripting</li><li class="summary">Stubs and mocks</li><li class="summary">Software interfaces (instead of real devices): help to simulate hardware failures</li><li class="summary">Interfaces for SUT state querying</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Design for automation should consider that:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Compatibility with existing test tools should be established early on.</li><li class="summary">Test tool compatibility with the SUT details is critical</li><li class="summary">Solutions may require development of program code and calls to APIs</li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<ul>

            </details>
        </li>
        <li id="gtaa" class="section">
            <details>
                <summary class="sectionsummary">The Generic Test Automation Architecture</summary>
                
<ul>
<li>
    <details class="subsection">
        <summary class="subsectionsummary">Introduction to gTAA</summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary">gTAA defines :</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Concept space</li><li class="summary">Layers</li><li class="summary">Components</li><li class="summary">Interfaces</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">gTAA is vendor-neutral and allows for a structured and modular approach to building a test automation solution by:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Defining the concept space, layers, services, and interfaces of a TAS 
                    to enable the realization of TASs 
                    by in-house as well as by externally developed components</li><li class="summary">Supporting simplified components for the effective and efficient development of test automation</li><li class="summary">Re-using test automation components wherever necessary</li><li class="summary">Easing the maintenance and evolution of TASs</li><li class="summary">Defining the essential features for a user of a TAS</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">TAS consists of:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Test environment (and its artifacts)</li><li class="summary">Test suites (a set of test cases including test data).</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">TAF provides:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Support for the realization of the test environment</li><li class="summary">Useful tools</li><li class="summary">Test harnesses</li><li class="summary">Supporting libraries</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">TAA principles to follow:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Single responsibility (component has only one reason to change)</li><li class="summary">Extension (component is closed for modification, open for extension)</li><li class="summary">Replacement (two components with the same functionality must be easily changeable)</li><li class="summary">Component segregation (one simple specific component is better than a large general one)</li><li class="summary">Dependency inversion (higher level components should not depend on the concrete implementation;
                    the TAF component should not depend on the test details)</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">gTAA horizontal layers(can be absent from the specific TAA)</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Test generation</li><li class="summary">Test definition</li><li class="summary">Test execution</li><li class="summary">Test adaptation</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">gTAA also has interfaces for:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Configuration management</li><li class="summary">Project management</li><li class="summary">Test management</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Test Generation Layer</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Supports tools for:<br> 
                    * manual test case design<br>
                    * developing, capturing, or deriving test data<br>
                    * automatically generating test cases from models that define the SUT and/or its environment<br></li><li class="summary">Components in this layer are used to:<br>
                * edit and navigate test suite structures <br>
                * relate test cases to test objectives or SUT requirements <br>
                * document the test design <br></li><li class="summary">Capabilities for automated test generation include:<br>
                * ability to model the SUT, its environment, and/or the test system<br>
                * ability to define test directives and to configure/parameterize test generation algorithms<br>
                * ability to trace the generated tests back to the model (elements)</li><li class="summary">Condsiderations for implementation:<br>
                * selection of manual or automated test generation<br>
                * selection of ,for example, requirements-based, data-based, scenario-based or behavior-based test generation<br>
                * selection of test generation strategies <br>
                * choice of the test selection strategy (practical coverage criteria, weights, risk assessments, etc)<br>
                </li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Test Definition Layer</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Supports tools for:<br>
                * specifying test cases (at a high and/or low level)<br>
                * defining test data for low-level test cases<br>
                * specifying test procedures for a test case or a set of test cases<br>
                * defining test scripts for the execution of the test cases<br>
                * providing access to test libraries as needed (for example in keyword-driven approaches<br></li><li class="summary">Components in this layer are used to:<br>
                * partition/constrain, parameterize or instantiate test data<br>
                * specify test sequences or fully-fledged test behaviors (including control statements and
                expressions), to parameterize and/or to group them<br>
                * document the test data, test cases and/or test procedures
                </li><li class="summary">Condsiderations for implementation:<br>
                * selection of data-driven, keyword-driven, pattern-based or model-driven test definition<br>
                * selection of notation for test definition <br>
                * selection of style guides and guidelines for the definition of high quality tests<br>
                * selection of test case repositories (spreadsheets, databases, files, etc.)<br>
                </li><li class="summary">Examples of notation for test definition:<br>
                * tables, state-based notation, stochastic notation,<br>
                * dataflow notation, business process notation, scenario-based notation<br>
                * spreadsheets, domain-specific test languages<br>
                * the Testing and Test Control Notation<br>
                * the UML Testing Profile (UTP)<br></li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Test Execution Layer</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Supports tools for:<br>
                 * executing test cases automatically<br>
                 * logging the test case executions<br>
                 * reporting the test results<br></li><li class="summary">Components in this layer provide the following capabilities:<br>
                * set up and tear down the SUT for test execution<br>
                * set up and tear down test suites (i.e., set of test cases including test data)<br>
                * configure and parameterize the test setup<br>
                * interpret both test data and test cases and transform them into executable scripts<br>
                * instrument the test system and/or the SUT for (filtered) logging of test execution and/or for fault
                injection<br>
                * analyze the SUT responses during test execution to steer subsequent test runs<br>
                * validate the SUT responses (comparison of expected and actual results) for automated test case
                execution results<br>
                * control the automated test execution in time<br></li><li class="summary">Condsiderations for implementation:<br>
                * selection of the test execution tool<br>
                * selection of interpretation or compilation approach (depends on the tool)<br>
                * selection of the implementation paradigm (imperative, functional, object-oriented, scripting or a tool-specific technology)<br>
                * selection of helper libraries to ease test execution<br>
                </li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Test Adaptation Layer</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Supports tools for:<br>
                * controlling the test harness<br>
                * interacting with the SUT<br>
                * monitoring the SUT<br>
                * simulating or emulating the SUT environment<br></li><li class="summary">Provides the following functionality:<br>
                * mediating between the technology-neutral test definitions and the specific technology requirements
                    of the SUT and the test devices<br>
                * applying different technology-specific adaptors to interact with the SUT<br>
                * distributing the test execution across multiple test devices/test interfaces or executing tests locally<br></li><li class="summary">Condsiderations for implementation:<br>
                * selection of test interfaces to the SUT
                * selection of tools to stimulate and observe the test interfaces
                * selection of tools to monitor the SUT during test execution
                * selection of tools to trace test execution (e.g., including the timing of the test execution)
                </li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Configuration management of a TAS</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">May include:<br>
                    * test models<br>
                    * test definitions/specifications including test data, test cases and libraries<br>
                    * test scripts<br>
                    * test execution engines and supplementary tools and components<br>
                    * test adaptors for the SUT<br>
                    * simulators and emulators for the SUT environment<br>
                    * test results and test reports<br></li><li class="summary">Should allow easy switching between versions</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Project management of a TAS and test management</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">TAS should provide easy access to the metrics and other reports to show to the managers</li><li class="summary">TAS should accept the SDLC of the SUT</li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<li>
    <details class="subsection">
        <summary class="subsectionsummary">TAA Design</summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary">TA approach considerations</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Which activity or phase of the test process should be automated (test generation stage is unique to TA)</li><li class="summary">Which test level should be supported</li><li class="summary">Which type of test should be supported</li><li class="summary">Which test role should be supported</li><li class="summary">Which software product(p.line, p.family) should be supported</li><li class="summary">Which SUT technologies should be supported</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Things to take into account to design a TAA</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Capture requirements needed to define an appropriate TAA</li><li class="summary">Compare and contrast different design/architecture approaches</li><li class="summary">Identify areas where abstraction can deliver benefits</li><li class="summary">Understand SUT technologies and how these interconnect with the TAS</li><li class="summary">Understand the SUT environment</li><li class="summary">Time and complexity for a given testware architecture implementation</li><li class="summary">Ease of use for a given testware architecture implementation</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Ways of automating test cases</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">to transform test cases directly into automated test scripts</li><li class="summary">to design test procedures and transform them into automated test scripts</li><li class="summary">to use a tool to translate test procedures into automated test scripts</li><li class="summary">to use a tool that generates automated test procedures and/or translates the test scripts directly from models</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Approaches for Automating Test Cases</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Capture/playback:<br>
                pros:<br>
                * easy to set up<br>
                cons:<br>
                * hard to maintain<br>
                * extremely flacky<br>
                * can't start before the SUT is ready<br>
                </li><li class="summary">Linear scripting:<br>
                pros:<br>
                * little preparation needed<br>
                * programming skills are not required most of the time<br>
                cons:<br>
                * repetition <br>
                * longer and non-modular scripts <br>
                * the effort depends on the number of steps <br>
                * requires effort to get familiar with the tool or language<br>
                </li><li class="summary">Structured scripting:<br>
                pros:<br>
                * usage of script libraries<br>
                * less effort for new tests <br>
                cons:<br>
                * larger initial effort <br>
                * programming skills required <br>
                * requires effort to manage scripts library<br>
                </li><li class="summary">Data-driven testing:<br>
                pros:<br>
                * less effort to create new tests due to control scripts <br>
                * more variations => larger coverage<br>
                * allows test analysts to specify tests via data files<br>
                cons:<br>
                * negative cases may be missed<br>
                * data files need to be managed<br>
                </li><li class="summary">Keyword-driven testing:<br>
                differences from DD-scripting:<br>
                * data files are called "test definitions"<br>
                * only one control script<br>
                * includes both data and "instructions"(keywords)
                pros:<br>
                * less effort for creating new tests <br>
                * test analysts may create tests<br>
                * tests can be defined in the terms of the high level actions<br>
                * keywords provide an abstraction <br>
                cons:<br>
                * requires a lot of effort if the tool does not support it<br>
                * requires care when deciding which keywords to implement<br>
                </li><li class="summary">Process-driven scripting:<br>
                concept: higher level abstraction 
                (small keywords vs longer procedures) <br>
                pros:<br>
                * workflow perspective <br>
                cons:<br>
                * SUT processes may be hard to grasp and describe<br>
                * requires care to implement correct processes <br>
                </li><li class="summary">Model-based testing:<br>
                pros:<br>
                * allows generating tests for different systems and technologies<br>
                * models are future-safe<br>
                * models require small changes when the SUT changes<br>
                * test design is incorporated in the test generation<br>
                cons:
                * requires a lot of expertise<br>
                * requires a lot of effort to model SUT<br>
                * requires adjustment is the test process<br>
                * not yet a mainstream<br>
                * models need to be tested and verified<br>
                </li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Technical considerations of the SUT</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Interfaces of the SUT</li><li class="summary">SUT data</li><li class="summary">SUT configurations</li><li class="summary">SUT standards and legal settings</li><li class="summary">Tools and tool environments used to develop the SUT</li><li class="summary">Test interfaces in the software product</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Considerations for Development/QA Processes</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Test execution control requirements</li><li class="summary">Reporting requirements</li><li class="summary">Role and access rights</li><li class="summary">Established tool landscape</li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<li>
    <details class="subsection">
        <summary class="subsectionsummary">TAS Development</summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary">Basic SDLC stages for a TAS</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Analyze</li><li class="summary">Design</li><li class="summary">Develop</li><li class="summary">Test</li><li class="summary">Deploy</li><li class="summary">Evolve</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">TAS development specific traits</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Compatibility with the SUT</li><li class="summary">Syncronization with the SUT</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Compatibility between the TAS and the SUT</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Process compatibility</li><li class="summary">Team compatibility(mindset, reviews, communication)</li><li class="summary">Technology compatibility(seamless interplay)</li><li class="summary">Tool compatibility</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Synchronization between TAS and SUT</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Synchronization of requirements:<br>
                * TAS has two types of the requirements:<br>
                  a) to it as a software<br>
                  b) to it as a testing tool of the SUT<br>
                * The second type of reqs need to be in sync
                  with the verion of the SUT<br>
                </li><li class="summary">Synchronization of development phases</li><li class="summary">Synchronization of defect tracking and confirmation testing</li><li class="summary">Synchronization of SUT and TAS evolution</li><li class="summary">Stages to sync:<br>
               * SUT design triggers TAS analysis<br>
               * if there is manual testing, its test deploy stage
                also support TAS analysis<br>
               * SUT test requires TAS deployment<br></li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Building Reuse into the TAS</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Reusable TAS artifacts can nclude:<br>
               * (parts of) test models of test goals, test scenarios, test components<br>
               * (parts of) test cases, test data, test procedures or test libraries themselves<br>
               * the test engine and/or test report framework<br>
               * the adaptors to the SUT components and/or interfaces<br></li><li class="summary">Factors that my increase the ability for reuse:<br>
               * following TAA and updating it as needed<br>
               * good TAS documentation <br> 
               * high quality of the TAS artifacts<br></li><li class="summary">Requires effort to measure, demonstrate benefits and evangelize</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Support for a Variety of Target Systems</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Variery of target systems = test different configurations</li><li class="summary">Different configurations may refer to:<br>
               * number and interconnection of SUT components<br>
               * SUT environments(software and hardware)<br>
               * technologies, prog. langs, OSes used to implement SUT<br>
               * libs and packages<br>
               * tools used to implement SUT<br>
               </li><li class="summary">The ability of a TAS to test different software product configurations is determined when the TAA is defined</li><li class="summary">Ways to handle the TAS variety and SUT variety:<br>
               * version/configuration control of both the TAS and SUT<br> 
               * TAS parametrization<br>
               </li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<ul>

            </details>
        </li>
        <li id="demployment" class="section">
            <details>
                <summary class="sectionsummary">Deployment Risks and Contingencies</summary>
                
<ul>
<li>
    <details class="subsection">
        <summary class="subsectionsummary">Selection of Test Automation Approach and Planning of Deployment/Rollout</summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary">Two main activities</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Pilot</li><li class="summary">Deployment</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Pilot steps to consider</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Identify a suitable project: <br>
                *do not select a critical project<br>
                *do not select a trivial project<br>
                *involve the necessary stakeholders n the selection process<br>
                *the SUT of the pilot must be representative<br>
                </li><li class="summary">Plan the pilot:<br>
                *reserve budget and resources<br>
                *define milestones<br>
                *get management commitment<br>
                </li><li class="summary">Conduct the pilot:<br>
                *check if TAS provides the functionality as expected<br>
                *check if the TAS and the existing process support each other<br>
                </li><li class="summary">Evaluate the pilot(use all stakeholders)</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Pilot objectives</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Learn more detail about the TAS</li><li class="summary">See how the TAS fits with existing processes, procedures and tools</li><li class="summary">See how TAS needs to adjust to the current processes or vice a versa</li><li class="summary">Design the automation interface to match the needs of the testers</li><li class="summary">Decide on standard ways of using, managing, storing and maintaining the TAS and the test assets</li><li class="summary">Identify metrics and measurement methods for test automation in use</li><li class="summary">Cost/benefit analysis</li><li class="summary">Required skills analysis</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Deployment steps to consider</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Identify initial target project(s)</li><li class="summary">Deploy the TAS in the selected projects</li><li class="summary">Monitor and evaluate the TAS in projects after 
                a pre-defined period</li><li class="summary">Rollout to the rest of the organization/projects</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Success factors for deployment</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">An incremental rollout</li><li class="summary">Adapting and improving processes to fit with the use of the TAS</li><li class="summary">Providing training and coaching/mentoring for new users</li><li class="summary">Defining usage guidelines</li><li class="summary">Implementing a way to gather information about the actual use</li><li class="summary">Monitoring TAS use, benefits and costs</li><li class="summary">Providing support for the test and development teams</li><li class="summary">Gathering lessons learned from all teams</li><li class="summary">Identifying and implementing steps for improvement</li><li class="summary">The deployment procedure must be well documented
                and version controlled</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">When is it better to deploy?</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Depends greatly on the phase of development of the software project</li><li class="summary">option A: in the beginning of the project</li><li class="summary">option B: when reaching a milestone(e.g., ode freeze or the end of a sprint.)</li><li class="summary">For TAS hotfixes the deployment may be out of sync with the project stage</li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<li>
    <details class="subsection">
        <summary class="subsectionsummary">Risk Assessment and Mitigation Strategies (deploymentwise)</summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary">Typical technical issues include:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Too much abstraction</li><li class="summary">Data tables can become too large/complex/cumbersome</li><li class="summary">TAS dependencies on external components(e.g, libs)</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Typical deployment project risks include:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Staffing issues</li><li class="summary">New SUT deliverables may cause the TAS to operate incorrectly</li><li class="summary">Delays in introducing automation</li><li class="summary">Delays in updating TAS based on the changes done to the SUT</li><li class="summary">The TAS cannot capture the (non-standard) objects it is intended to track</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Potential failure points of the TAS project include:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Migration to a different environment</li><li class="summary">Deployment to the target environment</li><li class="summary">New delivery from development</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Two cases of deploying a TAS</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Initial deployment</li><li class="summary">Maintenance deployment</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">First time deployment basic steps:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Define the infrastructure in which the TAS will run</li><li class="summary">Create the infrastructure for the TAS</li><li class="summary">Create a procedure for maintaining the TAS and its infrastructure</li><li class="summary">Create a procedure for maintaining the test suite that the TAS will execute</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">First time deployment risks include:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Total execution time of the test suite exceeds the expected time.
                <br>To mitigate:
                make sure that the test suite gets enough time to be executed</li><li class="summary">Installation and configuration issues with test environment. 
                <br>To mitigate:
                come up with an effective way to setup needed preconditions </li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Maintenance deployment steps:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Make an assessment of the changes in the new version of the TAS compared to the old one</li><li class="summary">Test the TAS for both new functionality and regressions</li><li class="summary">Check if the test suite needs to be adapted to the new version of the TAS</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Maintenance deployment risks include:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">The test suite needs to change to run on the updated TAS.
                <br>To mitigate:
                make the necessary changes to the test suite</li><li class="summary">Stubs, drivers and interfaces used in testing 
                need to change to fit with the updated TAS.
                <br>To mitigate:
                make the necessary changes to the test harness</li><li class="summary">The infrastructure needs to change to accommodate the updated TAS.
                <br>To mitigate:
                perform the necessary changes and test them with the updated TAS</li><li class="summary">The updated TAS has additional defects or performance issues.
                <br>To mitigate:
                perform an analysis of risks vs benefits. If the benefits outweight,
                deploy anyway with prper release notes</li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<li>
    <details class="subsection">
        <summary class="subsectionsummary">Test Automation Maintenance</summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary">Maintenance triggers</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Modifications</li><li class="summary">Migration</li><li class="summary">Retirement of the system</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Types of maintenance</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Preventive(making TAS futureproof)</li><li class="summary">Corrective(TAS failures fix)</li><li class="summary">Perfective(optimizations)</li><li class="summary">Adaptive(making TAS work with new techs and tools)</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Maintenance scope depends on:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">The size and complexity of the TAS</li><li class="summary">The size of the change</li><li class="summary">The risk of the change</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Good practices for maintaining the TAS:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">The deployment procedures and usage of the TAS must be clear and documented</li><li class="summary">The third party dependencies must be documented, together with drawbacks and known issues</li><li class="summary">The TAS must be modular, so parts of it can be easily replaced</li><li class="summary">The TAS must run in an environment that is replaceable or with replaceable components</li><li class="summary">The TAS must separate test scripts from the TAF itself</li><li class="summary">The TAS must run isolated from the development environment</li><li class="summary">The TAS together with the environment, test suite and testware artifacts must be under
                configuration management</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Third party components considerations:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">They need to be documented and their config. must be managed</li><li class="summary">A plan is needed in case changes to these components are required</li><li class="summary">Their licenses must be taken into account</li><li class="summary">Keep them up to date</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Benefits of naming standards and other conventions</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">TAS becomes easy to read, understand, change and maintain</li><li class="summary">Easier to introduce new people to the project</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Naming standards can refer to:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">variables</li><li class="summary">files</li><li class="summary">test scenarios</li><li class="summary">keywords</li><li class="summary">keyword parameters</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Other conventions can refer to:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">pre-requisites and post-actions for test execution</li><li class="summary">the content of the test data</li><li class="summary">the test environment</li><li class="summary">status of test execution</li><li class="summary">execution logs and reports</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Documentation considerations include:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Someone has to write it</li><li class="summary">Someone has to maintain it</li><li class="summary">What needs to be documented: the design, components, 
                integrations with third parties, dependencies and deployment
                procedures</li><li class="summary">writing of documentation should be a part of the development process</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Training material considerations include:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Well-written docs may be a basis to training materials</li><li class="summary">The maintenance of the training material consists of initially
                writing it and then reviewing it
                periodically</li><li class="summary">Training most likely happens towards the end of a lifecycle iteration of the SUT</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Training material is a combination of</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">functional specifications of the TAS</li><li class="summary">design and architecture of the TAS</li><li class="summary">deployment and maintenance of the TAS</li><li class="summary">usage of the TAS (user manual)</li><li class="summary">practical examples and exercises</li><li class="summary">tips and tricks</li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<ul>

            </details>
        </li>
        <li id="reporting" class="section">
            <details>
                <summary class="sectionsummary">Test Automation Reporting and Metrics</summary>
                
<ul>
<li>
    <details class="subsection">
        <summary class="subsectionsummary">Selection of TAS Metrics</summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary">Basic qualities:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">TAS metrics are separate from the SUT metrics</li><li class="summary">TAS metrics allow TAM and TAE to track progress and
                monitor the impact of changes to the TAS</li><li class="summary">Metrics can be external(impact on other activities) and internal
                (the effectiveness and efficiency of the TAS)</li><li class="summary">Trend metrics may be more valuable than a separate metric value</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">External TAS metrics</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Automation benefits</li><li class="summary">Effort to build automated tests</li><li class="summary">Effort to analyze automated test incidents</li><li class="summary">Effort to maintain automated tests</li><li class="summary">Ratio of failures to defects</li><li class="summary">Time to execute automated tests</li><li class="summary">Number of automated test cases</li><li class="summary">Number of pass and fail results</li><li class="summary">Number of false-fail and false-pass results</li><li class="summary">Code coverage</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Internal TAS metrics</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Tool scripting metrics</li><li class="summary">Automation code defect density</li><li class="summary">Speed and efficiency of TAS components</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Possible measures of automation benefits:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Number of hours of manual test effort saved</li><li class="summary">Reduction in time to perform regression testing</li><li class="summary">Number of additional cycles of test execution achieved</li><li class="summary">Number or percentage of additional tests executed</li><li class="summary">Percentage of automated test cases related to the entire set of test cases</li><li class="summary">Increase in coverage (requirements, functionality, structural)</li><li class="summary">Number of defects found earlier because of the TAS</li><li class="summary">Number of defects found because of the TAS which would not have been found by manual testing
                (e.g., reliability defects)</li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<li>
    <details class="subsection">
        <summary class="subsectionsummary">Implementation of Measurement</summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary">Features of automation that support measurement and report generation</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">When to log? Before, during and after test execution</li><li class="summary">What test level to log? Individual tests, sets of tests and an entire test suite</li><li class="summary">It's best to highlight the trends</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Integration with other third party tools</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Examples of the tools: spreadsheets, XML, documents, databases, report tools</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Visualization of results</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Examples of the visualization tools: dashboards, charts, graphs</li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<li>
    <details class="subsection">
        <summary class="subsectionsummary">Logging of the TAS and the SUT</summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary">TAS logging should include the following</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">Which test case is currently under execution, including start and end time.</li><li class="summary">The status of the test case execution</li><li class="summary">Details of the test log at a high level including timing information</li><li class="summary">Dynamic information about the SUT 
                    with the test case that revealed this info</li><li class="summary">A counter for repeated tests</li><li class="summary">Random data, generated by the test</li><li class="summary">Test case actions</li><li class="summary">Screenshots and other visual data</li><li class="summary">Details on failure</li><li class="summary">Storing logs persistently</li><li class="summary">Colors to distinguish different types of information</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">SUT logging:</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">In case of a failure SUT should log 
                    date and time stamps, 
                    source location of issue, error messages</li><li class="summary">SUT can log all user interaction</li><li class="summary">At startup of the system, configuration information should be logged to a file</li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<li>
    <details class="subsection">
        <summary class="subsectionsummary">Test Automation Reporting</summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary">Content of the reports</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">The execution results</li><li class="summary">The system being tested</li><li class="summary">The environment in which the tests were run</li>
        </div>
        <ol>
        </details></li><li><details class="keypoint">
        <summary class="keypointsummary">Publishing the reports</summary>
        <ol>
        <div class="pointslist">
        <li class="summary">The report should be published for everyone 
                    interested in the execution results</li><li class="summary">Reports can be:<br>
                * uploaded on a website<br>
                * sent to a mailing list<br>
                * uploaded to another tool<br>
                </li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<ul>

            </details>
        </li>
        <li id="mantoauto" class="section">
            <details>
                <summary class="sectionsummary">Transitioning Manual Testing to an Automated Environment</summary>
                
<ul>
<li>
    <details class="subsection">
        <summary class="subsectionsummary"></summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary"></summary>
        <ol>
        <div class="pointslist">
        <li class="summary"></li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<li>
    <details class="subsection">
        <summary class="subsectionsummary"></summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary"></summary>
        <ol>
        <div class="pointslist">
        <li class="summary"></li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<li>
    <details class="subsection">
        <summary class="subsectionsummary"></summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary"></summary>
        <ol>
        <div class="pointslist">
        <li class="summary"></li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<li>
    <details class="subsection">
        <summary class="subsectionsummary"></summary>
        <ul>
        <li><details class="keypoint">
        <summary class="keypointsummary"></summary>
        <ol>
        <div class="pointslist">
        <li class="summary"></li>
        </div>
        <ol>
        </details></li>
        </ul>
    </details></li>
<ul>

            </details>
        </li>
        <li id="verification" class="section">
            <details>
                <summary class="sectionsummary">Verifying the TAS</summary>
            </details>
        </li>
        <li id="improvement" class="section">
            <details>
                <summary class="sectionsummary">Continuous Improvement</summary>
            </details>
        </li>
    </ul>
    
</main>
</body>

</html>